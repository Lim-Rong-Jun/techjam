{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from opendatasets) (4.67.1)\n",
      "Collecting kaggle (from opendatasets)\n",
      "  Downloading kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click (from opendatasets)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Collecting bleach (from kaggle->opendatasets)\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (6.32.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
      "Collecting python-slugify (from kaggle->opendatasets)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (1.17.0)\n",
      "Collecting text-unidecode (from kaggle->opendatasets)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\thund\\desktop\\individual_project\\.venv\\lib\\site-packages (from kaggle->opendatasets) (2.5.0)\n",
      "Collecting webencodings (from kaggle->opendatasets)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, text-unidecode, python-slugify, click, bleach, kaggle, opendatasets\n",
      "\n",
      "   ----------- ---------------------------- 2/7 [python-slugify]\n",
      "   ----------------- ---------------------- 3/7 [click]\n",
      "   ---------------------- ----------------- 4/7 [bleach]\n",
      "   ---------------------- ----------------- 4/7 [bleach]\n",
      "   ---------------------- ----------------- 4/7 [bleach]\n",
      "   ---------------------- ----------------- 4/7 [bleach]\n",
      "   ---------------------------- ----------- 5/7 [kaggle]\n",
      "   ---------------------------- ----------- 5/7 [kaggle]\n",
      "   ---------------------------- ----------- 5/7 [kaggle]\n",
      "   ---------------------------- ----------- 5/7 [kaggle]\n",
      "   ---------------------------- ----------- 5/7 [kaggle]\n",
      "   ---------------------------------- ----- 6/7 [opendatasets]\n",
      "   ---------------------------------------- 7/7 [opendatasets]\n",
      "\n",
      "Successfully installed bleach-6.2.0 click-8.2.1 kaggle-1.7.4.5 opendatasets-0.1.22 python-slugify-8.0.4 text-unidecode-1.3 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn\n",
    "# !pip install numpy\n",
    "# !pip install transformers datasets accelerate evaluate sentencepiece protobuf huggingface_hub\n",
    "# !pip install opendatasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thund\\Desktop\\individual_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Core Transformers (models + tokenizers + pipelines)\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Hugging Face Datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Training accelerator (multi-GPU, TPU, CPU support)\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Evaluation metrics\n",
    "import evaluate\n",
    "\n",
    "# SentencePiece (used behind the scenes, rarely imported directly)\n",
    "# import sentencepiece  # usually not needed directly, required internally\n",
    "\n",
    "# Hugging Face Hub (upload/download models & datasets)\n",
    "from huggingface_hub import HfApi, notebook_login\n",
    "\n",
    "import opendatasets as od\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'opendatasets.datasets.C:/Users/thund/Downloads/archive/reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/thund/Downloads/archive/reviews.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thund\\Desktop\\individual_project\\.venv\\Lib\\site-packages\\opendatasets\\__init__.py:27\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(dataset_id_or_url, data_dir, force, dry_run, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(data_dir):\n\u001b[32m     26\u001b[39m     os.makedirs(data_dir)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m dataset = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopendatasets.datasets.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dry_run:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mThis is a dry run. URLs will be displayed but the files will not be downloaded.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1310\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'opendatasets.datasets.C:/Users/thund/Downloads/archive/reviews'"
     ]
    }
   ],
   "source": [
    "od.download(\"C:/Users/thund/Downloads/archive/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(\"C:/Users/thund/Downloads/archive/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pseudo labelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def rule_based_label(review):\n",
    "    if re.search(r\"http|www|discount|promo\", review.lower()):\n",
    "        return \"Advertisement\"\n",
    "    if any(word in review.lower() for word in [\"phone\", \"car\", \"delivery\"]):\n",
    "        return \"Irrelevant\"\n",
    "    if \"never been here\" in review.lower() or \"heard it's bad\" in review.lower():\n",
    "        return \"Rant_no_visit\"\n",
    "    return \"Trustworthy\"\n",
    "\n",
    "df[\"label\"] = df[\"review\"].apply(rule_based_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
